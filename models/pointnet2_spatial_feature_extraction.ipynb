{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from plyfile import PlyData\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "from models.pointnet2_utils import PointNetSetAbstraction, PointNetFeaturePropagation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPointCloudDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_files = [\n",
    "            os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.ply')\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        plydata = PlyData.read(self.data_files[idx])\n",
    "        vertex = plydata['vertex']\n",
    "        point_cloud = np.c_[vertex['x'], vertex['y'], vertex['z']].astype(np.float32)  # 转换为 float32\n",
    "\n",
    "        if point_cloud.shape[0] < 1024:\n",
    "            padding = np.zeros((1024 - point_cloud.shape[0], 3)).astype(np.float32)\n",
    "            point_cloud = np.vstack([point_cloud, padding])\n",
    "        elif point_cloud.shape[0] > 1024:\n",
    "            point_cloud = point_cloud[:1024, :]\n",
    "        \n",
    "        return torch.from_numpy(point_cloud).permute(1, 0)  # [3, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNet2FeatureExtractor, self).__init__()\n",
    "        # Encoder (Feature Extraction)\n",
    "        self.sa1 = PointNetSetAbstraction(1024, 0.1, 32, 6, [32, 32, 64], False)\n",
    "        self.sa2 = PointNetSetAbstraction(256, 0.2, 32, 67, [64, 64, 128], False)\n",
    "        self.sa3 = PointNetSetAbstraction(64, 0.4, 32, 131, [128, 128, 256], False)\n",
    "        self.sa4 = PointNetSetAbstraction(16, 0.8, 32, 259, [256, 256, 512], False) \n",
    "        \n",
    "        # Decoder (For Reconstruction)\n",
    "        self.fp4 = PointNetFeaturePropagation(768, [256, 256])\n",
    "        self.fp3 = PointNetFeaturePropagation(384, [256, 256])\n",
    "        self.fp2 = PointNetFeaturePropagation(320, [256, 128])\n",
    "        self.fp1 = PointNetFeaturePropagation(128, [128, 128])\n",
    "\n",
    "        self.final_conv = nn.Conv1d(128, 3, 1)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        l0_xyz = xyz[:, :3, :]  # [B, 3, N]\n",
    "        l0_points = xyz  # [B, 3, N]\n",
    "\n",
    "        # Feature Extraction\n",
    "        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        l4_xyz, l4_points = self.sa4(l3_xyz, l3_points)\n",
    "\n",
    "        # Feature Propagation (Decoding)\n",
    "        l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n",
    "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
    "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
    "        l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points)\n",
    "        \n",
    "        l0_points = self.final_conv(l0_points)  # [B, 3, N]\n",
    "\n",
    "        return l0_points  # [B, 3, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, dataloader, epochs, lr, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=len(dataloader), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "            for point_clouds in dataloader:\n",
    "                point_clouds = point_clouds.to(device)  # [B, 3, N]\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                reconstructed = model(point_clouds)  # [B, 3, N]\n",
    "                loss = F.mse_loss(reconstructed, point_clouds)  # Simple reconstruction loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                pbar.update(1)\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1} complete. Average Loss: {avg_loss}\")\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "    torch.save(model.state_dict(), \"pointnet2_feature_extractor.pth\")\n",
    "\n",
    "data_dir = \"/home/jerry/Pointnet_Pointnet2_pytorch/data\" \n",
    "dataset = HumanPointCloudDataset(data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PointNet2FeatureExtractor()\n",
    "train_model(model, dataloader, epochs=50, lr=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spatial_features(model, dataloader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for point_clouds in tqdm(dataloader, desc=\"Extracting Spatial Features\"):\n",
    "            point_clouds = point_clouds.to(device)  # [B, 3, N]\n",
    "            spatial_features = model(point_clouds)  # [B, feature_dim, N]\n",
    "            features.append(spatial_features.mean(dim=2))  # Global feature, [B, feature_dim]\n",
    "\n",
    "    return torch.cat(features, dim=0)  # [total_samples, feature_dim]\n",
    "\n",
    "spatial_model = PointNet2FeatureExtractor()\n",
    "spatial_model.load_state_dict(torch.load(\"your model.pth\"))\n",
    "\n",
    "spatial_features = extract_spatial_features(spatial_model, dataloader, device)\n",
    "torch.save(spatial_features, \"your.pt\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
